{"meta":{"title":"Wantoper","subtitle":"Blog","description":"","author":"Wantoper","url":"https://wantoper.github.io","root":"/"},"pages":[{"title":"分类","date":"2024-09-06T07:46:56.000Z","updated":"2024-09-06T08:02:55.168Z","comments":true,"path":"categorys/index.html","permalink":"https://wantoper.github.io/categorys/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2024-09-06T07:59:37.000Z","updated":"2024-09-06T08:02:41.887Z","comments":true,"path":"link/index.html","permalink":"https://wantoper.github.io/link/index.html","excerpt":"","text":""},{"title":"标签","date":"2024-09-06T07:47:16.000Z","updated":"2024-09-06T08:06:02.760Z","comments":true,"path":"tags/index.html","permalink":"https://wantoper.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"proxifier+fiddler抓包","slug":"proxifier-fiddler","date":"2024-10-03T03:04:01.000Z","updated":"2024-10-05T09:10:37.752Z","comments":true,"path":"2024/10/03/proxifier-fiddler/","permalink":"https://wantoper.github.io/2024/10/03/proxifier-fiddler/","excerpt":"","text":"Fiddler的设置Tools&gt;Options&gt;HTTPS Tools&gt;Options&gt;Connections F12可以关掉(File&gt;Capure traffic) ProxiffierProfile&gt;ProxyServers 这里设置成fiddler的代理 Profile&gt;Proxification Rules Profile&gt;Name Resolution 先取消勾选第一个 再勾选第二个","categories":[],"tags":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"https://wantoper.github.io/tags/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"}]},{"title":"某里两种版本的水果滑块训练——Yolov5","slug":"Ali_Fruit_Slider_Yolov5","date":"2024-09-30T06:32:53.000Z","updated":"2024-09-30T09:24:41.726Z","comments":true,"path":"2024/09/30/Ali_Fruit_Slider_Yolov5/","permalink":"https://wantoper.github.io/2024/09/30/Ali_Fruit_Slider_Yolov5/","excerpt":"","text":"前言本章不涉及任何逆向 仅对训练过程及识别方式进行讨论 总所周知 某里系有个著名的滑块 227 由于技术且未达到 所以先不搓js层次的，先对识别部分的先来练练手。某大佬说因为早期验证码内都是一些水果 所以就称为水果滑块。 验证码字体处的识别目前我所知的有两个版本 1.0最早期的比较简单在风控后就会出现 一般在登录可以见到 227多滑几次之后就会出现 1.0的识别相对简单可以直接降噪后用opencv识别 但是这里有点坑 直接用opencv读会很模糊后面会提到 2.0的目前没见到过 但是有一个接口会直接访问D 2.0的字体识别位置改为了两张图片反复切换，利用了视觉停留，看清了图片内容，想用截屏的方式是行不通的，两张图片都是带噪点的 单独拎出来是看不清的。 在接口处可以看到两张图片 两种方式采用opencv处理后用DDDDOCR进行识别 效果完全够用！ 1.0的文字处理把坑说在前头 123456789101112131415def opencv_img(base64_str): img_data = base64.b64decode(base64_str.split(&#x27;,&#x27;)[1]) nparr = np.frombuffer(img_data, np.uint8) img=cv2.cvtColor(img, cv2.COLOR_BGRA2BGR) if img.shape[2] == 4: b, g, r, a = cv2.split(img) white_background = np.ones((img.shape[0], img.shape[1], 3), dtype=np.uint8) * 255 for c in range(3): white_background[:, :, c] = b * (a / 255.0) + white_background[:, :, c] * (1 - a / 255.0) cv2.imshow(&#x27;Image with Transparency&#x27;, white_background) else: cv2.imshow(&#x27;Image&#x27;, img) cv2.waitKey(0) cv2.destroyAllWindows() 一开始我用这种方式去读取base64的背景图 发现效果是这样的 非常糊，人眼都不太好看清更别说ocr了 后面琢磨了一天始终没发现是读取图片代码的问题以为是opencv的处理问题，最后挨个试才发现这边需要保留透明通道 改用下面的代码后就行了 很高清无码！ 1img = cv2.imdecode(nparr, cv2.IMREAD_UNCHANGED) 这样再丢给ocr就嘚嘚得了 最后因为我需要打包成exe使用 为了压缩体积所以选择不使用Opencv 采用了PIL库 需要用Opencv来识别的可以丢给AI帮你生成一下 附上PIL的代码 123456789101112131415def deal_que_new_img(bin_image): ocr = DdddOcr(show_ad=False) img = Image.open(BytesIO(bin_image)).convert(&quot;RGBA&quot;) background = Image.new(&quot;RGBA&quot;, img.size, (255, 255, 255, 255)) img = Image.alpha_composite(background, img) image = img.crop((143, 0, img.size[0], img.size[1])) # image.show() ocr_res = ocr.classification(image).split(&#x27;后&#x27;)[0] logger.info(f&quot;识别结果：&#123;ocr_res&#125;&quot;) return ocr_res titleimg=&quot;base64&quot;que_img = b64decode(titleimg.split(&#x27;base64,&#x27;)[-1])queue = deal_que_new_img(que_img) 至此第一步就完成了 2.0的文字处理2.0的较为复杂一些一开始也思考了挺久，都准备上模型了，后面在公众号找到一个大佬的文章看了之后茅厕顿开 醍醐灌顶。 既然利用了视觉停留 在页面上图片反复跳转时字体还是蛮清晰的，所以将两张图片合二为一进行像素点遍历，保留两张图都存在的像素点，一张有 一张没有的像素点就去除掉。没有的话设为0白色 都有的话设为255黑色 123456789#同时遍历两张图片，找到不同的像素点 删除掉for y in range(original.shape[0]): for x in range(original.shape[1]): if np.any(original[y, x]) == 0 and np.any(original2[y, x]) == 0: original[y, x] = 0 original2[y, x] = 0 else: original[y, x] = 255 original2[y, x] = 255 效果还是挺好的 从”后”进行切割一下就行 滑块图的处理及训练图片长这样 多刷了几次我发现这样有规律 一般需要验证的物体的都在最后一个 例如两个熊猫 三个苹松鼠 一个鱼 只需要找到最后一个物体的坐标即可 有了这个规律就不需要语义模型来二次识别了 后期训练出yolo后遍历x坐标最大的即可获得出坐标位置。 模型训练训练前先要获取数据主要是图片和类别 获取直接就猛猛调接口就行了 也不建议太快 这里附上代码图 代码就不贴了 调接口把背景图保存，然后用前面的方法识别一下标题上的字获取类别，下来230多张就已经足够了 97%的准确率。类别的话可以多跑一些 基本上就能全跑完 打标注打标注我这里用label-studio+sam半自动标注 只需要点点点就行 可以看之前发过的一篇 我这里只标注了230张就用来训练了 最后导出成yolo格式的数据 Yolov5的训练预装Yolov5 Copy一份coco的修改成自己的yaml 简单修改一下train.py 的参数就可以运行 开始训练了 我这里只改了数据集的位置 其他都用默认的就好了 训练好后测试一下 识别效果还不错 123456789101112from loguru import loggerimport torchmodel=torch.hub.load(&quot;.&quot;, &quot;custom&quot;, path=&quot;./runs/train/exp12/weights/best.pt&quot;, source=&quot;local&quot;)img=r&#x27;./testdetect.png&#x27;reslut=model(img)print(type(reslut))#输出结果 类别，置信度，坐标logger.info(reslut.pandas().xyxy[0])reslut.show() 按照前面的规律就可以直接用了 1.使用获取标题 2.调用模型获得图片中所有分类 3.在模型结果中进行遍历 获取标题类别的所有坐标 4.遍历坐标x最大位置坐标就是了 最后还要加上当前框框的宽度就行了 迁移部署因为考虑了部署机器性能的原因我对其进行了迁移 转为Onnx直接在Yolo项目下对模型转换成onnx格式的 123456789101112import torchimport torchvisionfrom models.experimental import attempt_load# 加载模型权重model = attempt_load(&#x27;runs/train/exp12/weights/best.pt&#x27;, map_location=torch.device(&#x27;cpu&#x27;))# 设置模型为评估模式model.eval()# 准备一个示例输入input_tensor = torch.randn(1, 3, 640, 640) # 假设输入图像大小为 640x640# 导出模型#Lib\\site-packages\\torch\\nn\\modules\\ activation.pytorch.onnx.export(model, input_tensor, &#x27;AliFruit.onnx&#x27;,opset_version=11) 转为后封装一下模型 还有一些必要的函数nms xywh2xyxy extrack letterbox这些我都是从yolo里面Copy出来的 为了最简化 这里贴出一下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136class YOLOV5_ONNX(object): def __init__(self,onnx_path): &#x27;&#x27;&#x27;初始化onnx&#x27;&#x27;&#x27; self.onnx_session=onnxruntime.InferenceSession(onnx_path) self.classes=[&#x27;乌龟&#x27;,&#x27;企鹅&#x27;,&#x27;伞&#x27;,&#x27;免子&#x27;,&#x27;冰激凌&#x27;,&#x27;凤梨&#x27;,&#x27;包&#x27;,&#x27;南瓜&#x27;,&#x27;吉他&#x27;,&#x27;大象&#x27;,&#x27;太阳花&#x27;,&#x27;宇航员&#x27;,&#x27;帐蓬&#x27;,&#x27;帽子&#x27;,&#x27;房子&#x27;,&#x27;挂锁&#x27;,&#x27;杯子&#x27;,&#x27;松鼠&#x27;,&#x27;枕头&#x27;,&#x27;树&#x27;,&#x27;树袋熊&#x27;,&#x27;椅子&#x27;,&#x27;气球&#x27;,&#x27;汉堡包&#x27;,&#x27;熊猫&#x27;,&#x27;玫瑰花&#x27;,&#x27;瓢虫&#x27;,&#x27;瓶子&#x27;,&#x27;皇冠&#x27;,&#x27;篮子&#x27;,&#x27;耳机&#x27;,&#x27;花盆&#x27;,&#x27;苹果&#x27;,&#x27;草莓&#x27;,&#x27;蘑菇&#x27;,&#x27;蛋糕&#x27;,&#x27;蝴蝶&#x27;,&#x27;裙子&#x27;,&#x27;足球&#x27;,&#x27;车&#x27;,&#x27;轮胎&#x27;,&#x27;铲土机&#x27;,&#x27;闹钟&#x27;,&#x27;鞋&#x27;,&#x27;马&#x27;,&#x27;鱼&#x27;,&#x27;鸟&#x27;,&#x27;鸭子&#x27;] def letterbox(self,img, new_shape=(640, 640), color=(114, 114, 114), auto=False, scaleFill=False, scaleup=True,stride=32): &#x27;&#x27;&#x27;图片归一化&#x27;&#x27;&#x27; # Resize and pad image while meeting stride-multiple constraints shape = img.shape[:2] # current shape [height, width] if isinstance(new_shape, int): new_shape = (new_shape, new_shape) # Scale ratio (new / old) r = min(new_shape[0] / shape[0], new_shape[1] / shape[1]) if not scaleup: # only scale down, do not scale up (for better test mAP) r = min(r, 1.0) # Compute padding ratio = r, r # width, height ratios new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r)) dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1] # wh padding if auto: # minimum rectangle dw, dh = np.mod(dw, stride), np.mod(dh, stride) # wh padding elif scaleFill: # stretch dw, dh = 0.0, 0.0 new_unpad = (new_shape[1], new_shape[0]) ratio = new_shape[1] / shape[1], new_shape[0] / shape[0] # width, height ratios dw /= 2 # divide padding into 2 sides dh /= 2 if shape[::-1] != new_unpad: # resize img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR) top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1)) left, right = int(round(dw - 0.1)), int(round(dw + 0.1)) img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color) # add border return img, ratio, (dw, dh) def infer(self,src_img): &#x27;&#x27;&#x27;执行前向操作预测输出&#x27;&#x27;&#x27; or_img = self.letterbox(src_img, (640, 640), stride=32)[0] # BGR2RGB img = or_img[:, :, ::-1].transpose(2, 0, 1) # BGR2RGB和HWC2CHW img = img.astype(dtype=np.float32) img /= 255.0 img = np.expand_dims(img, axis=0) pred = self.onnx_session.run(None, &#123;self.onnx_session.get_inputs()[0].name: img&#125;)[0] outbox = model.extrack(pred, 0.5, 0.5) # draw(or_img, outbox) # cv2.imshow(&#x27;result&#x27;, or_img) # cv2.waitKey(0) return outbox # dets: array [x,6] 6个值分别为x1,y1,x2,y2,score,class # thresh: 阈值 def nms(self, dets, thresh): x1 = dets[:, 0] y1 = dets[:, 1] x2 = dets[:, 2] y2 = dets[:, 3] # ------------------------------------------------------- # 计算框的面积 # 置信度从大到小排序 # ------------------------------------------------------- areas = (y2 - y1 + 1) * (x2 - x1 + 1) # 公式=长*宽 scores = dets[:, 4] keep = [] index = scores.argsort()[::-1] while index.size &gt; 0: i = index[0] keep.append(i) # ------------------------------------------------------- # 计算相交面积 # 1.相交 # 2.不相交 # ------------------------------------------------------- x11 = np.maximum(x1[i], x1[index[1:]]) y11 = np.maximum(y1[i], y1[index[1:]]) x22 = np.minimum(x2[i], x2[index[1:]]) y22 = np.minimum(y2[i], y2[index[1:]]) w = np.maximum(0, x22 - x11 + 1) h = np.maximum(0, y22 - y11 + 1) overlaps = w * h # ------------------------------------------------------- # 计算该框与其它框的IOU，去除掉重复的框，即IOU值大的框 # IOU小于thresh的框保留下来 # ------------------------------------------------------- ious = overlaps / (areas[i] + areas[index[1:]] - overlaps) idx = np.where(ious &lt;= thresh)[0] index = index[idx + 1] return keep def xywh2xyxy(self, x): # [x, y, w, h] to [x1, y1, x2, y2] y = np.copy(x) y[:, 0] = x[:, 0] - x[:, 2] / 2 # x=x-w/2 y[:, 1] = x[:, 1] - x[:, 3] / 2 # y=y-h/2 y[:, 2] = x[:, 0] + x[:, 2] / 2 # x=x+w/2 y[:, 3] = x[:, 1] + x[:, 3] / 2 # y=y+h/2 return y def extrack(self, output, conf_thres=0.5, iou_thres=0.5): output = np.squeeze(output) # 过滤掉置信度小于0.5的框 outputcheck = output[..., 4] &gt; conf_thres output = output[outputcheck] # 获取每个框最大置信度的类别 放到第6列 x,y,w,h,conf,class····· for i in range(len(output)): output[i][5] = np.argmax(output[i][5:]) # 只取前6列 x,y,w,h,conf,class output = output[..., 0:6] # 将x,y,w,h转换为x1,y1,x2,y2 output = self.xywh2xyxy(output) # 过滤掉重复的框 output1 = self.nms(output, iou_thres) outputlist = [] for i in output1: outputlist.append(output[i]) outputlist = np.array(outputlist) return outputlist if __name__==&quot;__main__&quot;: model = YOLOV5_ONNX(onnx_path=&quot;./AliFruit.onnx&quot;) background_img=&quot;base64&quot; back_img = b64decode(background_img.split(&#x27;base64,&#x27;)[-1]) back_img = cv2.imdecode(np.frombuffer(back_img, np.uint8), cv2.IMREAD_COLOR) result = model.infer(back_img).tolist() 最后我使用了flask来部署调用 这里附上全部代码（1.0版本的） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205#encoding:utf-8import timefrom io import BytesIOimport onnxruntimefrom loguru import loggerfrom base64 import b64decodeimport numpy as npimport cv2from PIL import Imagefrom ddddocr import DdddOcrfrom flask import Flask, requestlogger.add(&quot;Flask_Web.log&quot;, rotation=&quot;10 MB&quot;, encoding=&quot;utf-8&quot;, level=&quot;INFO&quot;)app = Flask(__name__)class YOLOV5_ONNX(object): def __init__(self,onnx_path): &#x27;&#x27;&#x27;初始化onnx&#x27;&#x27;&#x27; self.onnx_session=onnxruntime.InferenceSession(onnx_path) self.classes=[&#x27;乌龟&#x27;,&#x27;企鹅&#x27;,&#x27;伞&#x27;,&#x27;免子&#x27;,&#x27;冰激凌&#x27;,&#x27;凤梨&#x27;,&#x27;包&#x27;,&#x27;南瓜&#x27;,&#x27;吉他&#x27;,&#x27;大象&#x27;,&#x27;太阳花&#x27;,&#x27;宇航员&#x27;,&#x27;帐蓬&#x27;,&#x27;帽子&#x27;,&#x27;房子&#x27;,&#x27;挂锁&#x27;,&#x27;杯子&#x27;,&#x27;松鼠&#x27;,&#x27;枕头&#x27;,&#x27;树&#x27;,&#x27;树袋熊&#x27;,&#x27;椅子&#x27;,&#x27;气球&#x27;,&#x27;汉堡包&#x27;,&#x27;熊猫&#x27;,&#x27;玫瑰花&#x27;,&#x27;瓢虫&#x27;,&#x27;瓶子&#x27;,&#x27;皇冠&#x27;,&#x27;篮子&#x27;,&#x27;耳机&#x27;,&#x27;花盆&#x27;,&#x27;苹果&#x27;,&#x27;草莓&#x27;,&#x27;蘑菇&#x27;,&#x27;蛋糕&#x27;,&#x27;蝴蝶&#x27;,&#x27;裙子&#x27;,&#x27;足球&#x27;,&#x27;车&#x27;,&#x27;轮胎&#x27;,&#x27;铲土机&#x27;,&#x27;闹钟&#x27;,&#x27;鞋&#x27;,&#x27;马&#x27;,&#x27;鱼&#x27;,&#x27;鸟&#x27;,&#x27;鸭子&#x27;] def letterbox(self,img, new_shape=(640, 640), color=(114, 114, 114), auto=False, scaleFill=False, scaleup=True,stride=32): &#x27;&#x27;&#x27;图片归一化&#x27;&#x27;&#x27; # Resize and pad image while meeting stride-multiple constraints shape = img.shape[:2] # current shape [height, width] if isinstance(new_shape, int): new_shape = (new_shape, new_shape) # Scale ratio (new / old) r = min(new_shape[0] / shape[0], new_shape[1] / shape[1]) if not scaleup: # only scale down, do not scale up (for better test mAP) r = min(r, 1.0) # Compute padding ratio = r, r # width, height ratios new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r)) dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1] # wh padding if auto: # minimum rectangle dw, dh = np.mod(dw, stride), np.mod(dh, stride) # wh padding elif scaleFill: # stretch dw, dh = 0.0, 0.0 new_unpad = (new_shape[1], new_shape[0]) ratio = new_shape[1] / shape[1], new_shape[0] / shape[0] # width, height ratios dw /= 2 # divide padding into 2 sides dh /= 2 if shape[::-1] != new_unpad: # resize img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR) top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1)) left, right = int(round(dw - 0.1)), int(round(dw + 0.1)) img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color) # add border return img, ratio, (dw, dh) def infer(self,src_img): &#x27;&#x27;&#x27;执行前向操作预测输出&#x27;&#x27;&#x27; or_img = self.letterbox(src_img, (640, 640), stride=32)[0] # BGR2RGB img = or_img[:, :, ::-1].transpose(2, 0, 1) # BGR2RGB和HWC2CHW img = img.astype(dtype=np.float32) img /= 255.0 img = np.expand_dims(img, axis=0) pred = self.onnx_session.run(None, &#123;self.onnx_session.get_inputs()[0].name: img&#125;)[0] outbox = model.extrack(pred, 0.5, 0.5) # draw(or_img, outbox) # cv2.imshow(&#x27;result&#x27;, or_img) # cv2.waitKey(0) return outbox # dets: array [x,6] 6个值分别为x1,y1,x2,y2,score,class # thresh: 阈值 def nms(self, dets, thresh): x1 = dets[:, 0] y1 = dets[:, 1] x2 = dets[:, 2] y2 = dets[:, 3] # ------------------------------------------------------- # 计算框的面积 # 置信度从大到小排序 # ------------------------------------------------------- areas = (y2 - y1 + 1) * (x2 - x1 + 1) # 公式=长*宽 scores = dets[:, 4] keep = [] index = scores.argsort()[::-1] while index.size &gt; 0: i = index[0] keep.append(i) # ------------------------------------------------------- # 计算相交面积 # 1.相交 # 2.不相交 # ------------------------------------------------------- x11 = np.maximum(x1[i], x1[index[1:]]) y11 = np.maximum(y1[i], y1[index[1:]]) x22 = np.minimum(x2[i], x2[index[1:]]) y22 = np.minimum(y2[i], y2[index[1:]]) w = np.maximum(0, x22 - x11 + 1) h = np.maximum(0, y22 - y11 + 1) overlaps = w * h # ------------------------------------------------------- # 计算该框与其它框的IOU，去除掉重复的框，即IOU值大的框 # IOU小于thresh的框保留下来 # ------------------------------------------------------- ious = overlaps / (areas[i] + areas[index[1:]] - overlaps) idx = np.where(ious &lt;= thresh)[0] index = index[idx + 1] return keep def xywh2xyxy(self, x): # [x, y, w, h] to [x1, y1, x2, y2] y = np.copy(x) y[:, 0] = x[:, 0] - x[:, 2] / 2 # x=x-w/2 y[:, 1] = x[:, 1] - x[:, 3] / 2 # y=y-h/2 y[:, 2] = x[:, 0] + x[:, 2] / 2 # x=x+w/2 y[:, 3] = x[:, 1] + x[:, 3] / 2 # y=y+h/2 return y def extrack(self, output, conf_thres=0.5, iou_thres=0.5): output = np.squeeze(output) # 过滤掉置信度小于0.5的框 outputcheck = output[..., 4] &gt; conf_thres output = output[outputcheck] # 获取每个框最大置信度的类别 放到第6列 x,y,w,h,conf,class····· for i in range(len(output)): output[i][5] = np.argmax(output[i][5:]) # 只取前6列 x,y,w,h,conf,class output = output[..., 0:6] # 将x,y,w,h转换为x1,y1,x2,y2 output = self.xywh2xyxy(output) # 过滤掉重复的框 output1 = self.nms(output, iou_thres) outputlist = [] for i in output1: outputlist.append(output[i]) outputlist = np.array(outputlist) return outputlistdef deal_que_new_img(bin_image): img = Image.open(BytesIO(bin_image)).convert(&quot;RGBA&quot;) background = Image.new(&quot;RGBA&quot;, img.size, (255, 255, 255, 255)) img = Image.alpha_composite(background, img) image = img.crop((143, 0, img.size[0], img.size[1])) # image.show() ocr_res = ocr.classification(image).split(&#x27;后&#x27;)[0] logger.info(f&quot;识别结果：&#123;ocr_res&#125;&quot;) return ocr_resdef draw(image, box_data): # ------------------------------------------------------- # 取整，方便画框 # ------------------------------------------------------- boxes = box_data[..., :4].astype(np.int32) scores = box_data[..., 4] # print(scores) classes = box_data[..., 5].astype(np.int32) for box, score, cl in zip(boxes, scores, classes): top, left, right, bottom = box cv2.rectangle(image, (top, left), (right, bottom), (255, 0, 0), 2) cv2.putText(image, &#x27;&#123;0&#125; &#123;1:.2f&#125;&#x27;.format(0, score), (top, left), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2, lineType=cv2.LINE_AA)@app.route(&#x27;/getdetectresult&#x27;, methods=[&#x27;POST&#x27;])def getdetectresult(): notitme=time.time() try: jsondata = request.json print(jsondata) title_img1 = jsondata.get(&quot;title_img1&quot;) print(title_img1) background_img = jsondata.get(&quot;background_img&quot;) que_img = b64decode(title_img1.split(&#x27;base64,&#x27;)[-1]) queue = deal_que_new_img(que_img) back_img = b64decode(background_img.split(&#x27;base64,&#x27;)[-1]) back_img = cv2.imdecode(np.frombuffer(back_img, np.uint8), cv2.IMREAD_COLOR) result = model.infer(back_img).tolist() queid = model.classes.index(queue.split(&quot;个&quot;)[-1]) # print(result) rere = [i for i in result if int(i[5]) == queid] rere.sort(key=lambda x: x[2]) drawdict = rere[-1] result_x = int(drawdict[2] / 640 * back_img.shape[1]) logger.info(f&quot;&#123;queue&#125;\\t&#123;result_x&#125;\\t&#123;result&#125;&quot;) except Exception as e: logger.error(e) return &#123;&quot;code&quot;:-1,&quot;msg&quot;:&quot;未识别到&quot;,&quot;data&quot;:[]&#125; logger.info(f&quot;耗时：&#123;time.time()-notitme&#125;&quot;) return &#123;&quot;code&quot;:0,&quot;msg&quot;:&quot;识别成功&quot;,&quot;data&quot;:&#123;&quot;x&quot;:result_x,&quot;queue&quot;:queue,&quot;result_detect&quot;:result&#125;&#125;if __name__==&quot;__main__&quot;: ocr = DdddOcr(show_ad=False) model = YOLOV5_ONNX(onnx_path=&quot;./AliFruit.onnx&quot;) app.run(host=&#x27;0.0.0.0&#x27;, port=8848, debug=True)","categories":[{"name":"Python","slug":"Python","permalink":"https://wantoper.github.io/categories/Python/"}],"tags":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"https://wantoper.github.io/tags/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"}]},{"title":"部署playground label_studio+sam模型 自动打标注","slug":"label-studio-sam","date":"2024-09-12T08:12:37.000Z","updated":"2024-09-12T08:56:24.212Z","comments":true,"path":"2024/09/12/label-studio-sam/","permalink":"https://wantoper.github.io/2024/09/12/label-studio-sam/","excerpt":"","text":"前言参考文章:小白学视觉 项目仓库:playground 前些天做了一个验证码识别的小项目 用到了yolo来训练，没有现成的数据所以需要自己手动打标注，在我尝试用手动工具打了几十张之后感觉怀疑人生！瞬间就不想干了。后面去找了半自动标注的相关项目，发现这个label_studio+sam的还可以。效果图先放上 SAM (Segment Anything) 是 Meta AI 推出的分割一切的模型。 Label Studio 是一款优秀的标注软件，覆盖图像分类、目标检测、分割等领域数据集标注的功能。 环境搭建我这里选择使用conda进行安装python环境 版本尽量选择3.9以上的，不然可能会有很多环境不适配。 12conda create -n rtmdet-sam python=3.9 -yconda activate rtmdet-sam 安装Pytorch 这里安装可能要很久 可以选择换源下载 12345678# Linux and Windows CUDA 11.3版本pip install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu113/torch_stable.html# Linux and Windows CPU 版本pip install torch==1.10.1+cpu torchvision==0.11.2+cpu torchaudio==0.10.1 -f https://download.pytorch.org/whl/cpu/torch_stable.html# OSXpip install torch==1.10.1 torchvision==0.11.2 torchaudio==0.10.1 拉取项目 1git clone https://github.com/open-mmlab/playground 安装sam预训练模型 12345678cd path/to/playground/label_anythingpip install opencv-python pycocotools matplotlib onnxruntime onnxpip install git+https://github.com/facebookresearch/segment-anything.gitwget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth# 如果想要分割的效果好请使用 sam_vit_h_4b8939.pth 权重# wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth# wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth 安装 Label-Studio 和 label-studio-ml-backend 12345# sudo apt install libpq-dev python3-dev # Note：如果使用 Label Studio 1.7.2 版本需要安装 `libpq-dev` 和 `python3-dev` 依赖。# 安装 label-studio 需要一段时间,如果找不到版本请使用官方源pip install label-studio==1.7.3pip install label-studio-ml==1.0.9 到这里环境依赖都安装完了 启动项目1234567891011121314#切换condaconda activate rtmdet-sam#sam 启动！ 使用sam_vit_b_01ec64.pthlabel-studio-ml start sam --port 8003 --with sam_config=vit_b sam_checkpoint_file=./sam_vit_b_01ec64.pth out_mask=True out_bbox=True device=cuda:0 #sam 启动！ 使用sam_hq_vit_h.pthlabel-studio-ml start sam --port 8003 --with sam_config=vit_h sam_checkpoint_file=./sam_hq_vit_h.pth out_mask=False out_bbox=False device=cuda:0 #上面二选一就行#label-studio 启动!set ML_TIMEOUT_SETUP=40label-studio start 其中可能会报点错 安装一下依赖即可 启动后使用浏览器访问http://localhost:8080/ 即可进入web 使用进入网站后先注册一个用户，这个用户仅仅是本地的。 注册后创建一个OpenMMLabPlayGround 项目 在Data Import中导入数据集 点击Save即可创建项目 进入项目在右上角在 Settings/Labeling Interface 中配置 Label-Studio 关键点和 Mask 标注。 12345678910111213141516171819&lt;View&gt; &lt;Image name=&quot;image&quot; value=&quot;$image&quot; zoom=&quot;true&quot;/&gt; &lt;KeyPointLabels name=&quot;KeyPointLabels&quot; toName=&quot;image&quot;&gt; &lt;Label value=&quot;cat&quot; smart=&quot;true&quot; background=&quot;#e51515&quot; showInline=&quot;true&quot;/&gt; &lt;Label value=&quot;person&quot; smart=&quot;true&quot; background=&quot;#412cdd&quot; showInline=&quot;true&quot;/&gt; &lt;/KeyPointLabels&gt; &lt;RectangleLabels name=&quot;RectangleLabels&quot; toName=&quot;image&quot;&gt; &lt;Label value=&quot;cat&quot; background=&quot;#FF0000&quot;/&gt; &lt;Label value=&quot;person&quot; background=&quot;#0d14d3&quot;/&gt; &lt;/RectangleLabels&gt; &lt;PolygonLabels name=&quot;PolygonLabels&quot; toName=&quot;image&quot;&gt; &lt;Label value=&quot;cat&quot; background=&quot;#FF0000&quot;/&gt; &lt;Label value=&quot;person&quot; background=&quot;#0d14d3&quot;/&gt; &lt;/PolygonLabels&gt; &lt;BrushLabels name=&quot;BrushLabels&quot; toName=&quot;image&quot;&gt; &lt;Label value=&quot;cat&quot; background=&quot;#FF0000&quot;/&gt; &lt;Label value=&quot;person&quot; background=&quot;#0d14d3&quot;/&gt; &lt;/BrushLabels&gt;&lt;/View&gt; &lt;Label value=&quot;cat&quot; background=&quot;#FF0000&quot;/&gt;代表一个分类的标签，这里需要手动添加，分类多的话就可以使用工具来做。 其中 KeyPointLabels 为关键点标注，BrushLabels 为 Mask 标注，PolygonLabels 为外接多边形标注，RectangleLabels 为矩形标注。 配置自动化标注模型然后在设置中点击 Add Model 添加 OpenMMLabPlayGround 后端推理服务,设置好 SAM 后端推理服务的 URL，并打开 Use for interactive preannotations 并点击 Validate and Save。 Connected连接上就说明配置成功了！ 重新进入项目后点击Label All Tasks就可开始标注了 需要打开 Auto-Annotation 的开关，并建议勾选 Auto accept annotation suggestions,并点击右侧 Smart 工具，切换到 Point 后，选择下方需要标注的物体标签，这里选择 cat。如果是 BBox 作为提示词请将 Smart 工具切换到 Rectangle。 标注完成后可以选择导出","categories":[],"tags":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"https://wantoper.github.io/tags/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"}]},{"title":"记一次某东登录滑块 识别+轨迹","slug":"JD-Slider","date":"2024-09-10T02:38:07.000Z","updated":"2024-10-05T09:10:37.736Z","comments":true,"path":"2024/09/10/JD-Slider/","permalink":"https://wantoper.github.io/2024/09/10/JD-Slider/","excerpt":"","text":"前言前段时间闲来无事想找个带轨迹的滑块练练手，正好找到一个算是入门级的小demo了，近期有空整理了技术点更新上来记录一下~~ 这个滑块主要难点就是轨迹，只要轨迹过了剩下的都很轻松，所以作为滑块类入门的demo最合适了~ 把这个滑块过了再去对抗更高级些的就有思路了。 入口就不放了 从官网进入手动登录就会弹 接口分析点击一下刷新验证码可以看到这个包 其中bg为后面的背景图，patch为滑块的小拼图。后续可以使用模板匹配来进行识别。 验证接口 手动滑一次可以看到该包 c就是前面接口中返回的challenge的值，d为轨迹的密文，下面进行逆向分析。w的值就较为重要了，应该是bg图片在页面上的大小，一开始我没注意到这个值，导致折磨了我很久，后续的轨迹以及识别位置对了也会返回失败，后面才反应出来去分析这个值。 返回值的说明先写在前头 返回值0为验证失败jsonp_xxxxx(&#123;&quot;success&quot;:&quot;0&quot;,&quot;message&quot;:&quot;fail&quot;,&quot;nextVerify&quot;:&quot;SLIDE_VERIFY&quot;&#125;) 返回值1为识别滑块的距离正确 validate有返回值说明验证通过jsonp_xxxxx(&#123;&quot;success&quot;:&quot;1&quot;,&quot;message&quot;:&quot;success&quot;,&quot;validate&quot;:&quot;xxxxxxx&quot;,&quot;nextVerify&quot;:&quot;NULL_VERIFY&quot;&#125;) 返回这个即为 轨迹校验失败jsonp_xxxxx(&#123;&quot;u&quot;:&quot;2&quot;,&quot;success&quot;:&quot;0&quot;,&quot;message&quot;:&quot;refuse&quot;,&quot;nextVerify&quot;:&quot;SLIDE_VERIFY&quot;&#125;) 轨迹的加密首先定位下断点，方法有很多种，我这里通过松开鼠标事件来定位 然后滑动滑块，随便找个位置松开就会断住 断住后发现没有混淆，通过代码可以进行猜测分析。 a[&#39;mousePos&#39;] 很明显就是轨迹的数组了 在第0和第1组的x有一个偏差，这里大概是22-25左右。 后续跟入submit函数看他是怎么对该数组进行加密的 进入submit函数后通过控制台输出来慢慢分析，发现d参数就在下面 通过a[&#39;getCoordinate&#39;](b)来进行生成密文 再进入a[&#39;getCoordinate&#39;]()函数 非常简单的代码，扣下来就行，后面缺啥补啥，大概补2-3个函数即可。 滑块距离的识别我想到的方案大致有两种一种是使用模型来识别，还有一种是使用Opencv来进行模板匹配识别。 我这里为了方便省事采用第二种，正确率大概为96%左右 已经很够用了！ 给出参考代码如下 12345678910111213141516171819202122232425262728293031323334353637import base64import cv2import numpy as npdef _tran_canny(image): &quot;&quot;&quot;消除噪声&quot;&quot;&quot; image = cv2.GaussianBlur(image, (3, 3), 0) return cv2.Canny(image, 50, 150)def draw_slider(image, template, top_left, max_loc): w, h = image.shape[1], image.shape[0] cv2.line(template, (top_left, 0), (top_left, 94), (0, 255, 0), 1) bottom_right = (max_loc[0] + w, max_loc[1] + h) cv2.rectangle(template, max_loc, bottom_right, (0, 255, 0), 1) cv2.imshow(&#x27;Detected&#x27;, template) cv2.waitKey(0)def detect_displacement(img_slider_path, image_background_path): x = base64.b64decode(img_slider_path) img_array = np.frombuffer(x, np.uint8) image = cv2.imdecode(img_array, cv2.COLOR_RGB2BGR) image = cv2.resize(image, (34, 34)) y = base64.b64decode(image_background_path) img_array2 = np.frombuffer(y, np.uint8) template = cv2.imdecode(img_array2, cv2.COLOR_RGB2BGR) #这里的242即为前面的w值 在F12源代码中取即可 template = cv2.resize(template, (242, 94)) #进行模板匹配 res = cv2.matchTemplate(_tran_canny(image), _tran_canny(template), cv2.TM_CCOEFF_NORMED) min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res) #计算出来需要加上一点偏移 就是前面分析的轨迹数组中的第0项与第1项的插值 top_left = round(max_loc[0])+11 print(top_left,max_loc) draw_slider(image, template, top_left, max_loc) return top_left 这里需要注意的是对template进行resize为242 对应上前面的w值，而后续的top_left计算出后需要加上一点偏移，偏移量就是前面分析的轨迹数组中的第0项与第1项的差值，我这里取的为11，一定要与轨迹数组中的对应。 滑动轨迹这就到了整个流程中最难的一步了，轨迹校验上了模型，我尝试了几种简单的生成都过不了，贝塞尔曲线 随机生成模拟 基于前两种后加柏林噪声。 本来想研究出一套能过的算法，久经折磨之后还是放弃了，使用手动采集轨迹后面再进行裁剪合成。这样通过率大概只有70%-80%左右 也勉强算够用了。 生成轨迹的参考代码 1234567891011121314151617181920212223242526import randomimport timetrack=[[1177,339,1723343514943],[1188,370,1723343514943],[1189,370,1723343515185],[1191,371,1723343515192],[1192,371,1723343515200],[1194,371,1723343515208],[1195,372,1723343515225],[1196,372,1723343515233],[1197,372,1723343515248],[1199,372,1723343515264],[1200,373,1723343515280],[1201,373,1723343515288],[1202,373,1723343515304],[1203,373,1723343515385],[1204,373,1723343515401],[1205,373,1723343515417],[1205,374,1723343515425],[1206,374,1723343515432],[1207,374,1723343515440],[1208,374,1723343515448],[1209,374,1723343515456],[1210,374,1723343515464],[1211,374,1723343515472],[1212,374,1723343515480],[1213,374,1723343515488],[1214,374,1723343515496],[1215,374,1723343515504],[1216,374,1723343515520],[1217,374,1723343515530],[1218,374,1723343515553],[1219,374,1723343515577],[1220,374,1723343515593],[1221,374,1723343515609],[1222,374,1723343515641],[1223,374,1723343515665],[1224,374,1723343515674],[1225,374,1723343515680],[1226,374,1723343515689],[1227,374,1723343515696],[1229,374,1723343515704],[1230,375,1723343515712],[1232,375,1723343515729],[1234,375,1723343515737],[1235,376,1723343515744],[1237,376,1723343515752],[1239,376,1723343515760],[1241,377,1723343515768],[1242,377,1723343515776],[1244,377,1723343515784],[1245,377,1723343515793],[1246,377,1723343515800],[1247,377,1723343515809],[1248,377,1723343515816],[1249,377,1723343515824],[1250,377,1723343515832],[1251,377,1723343515840],[1252,377,1723343515848],[1253,377,1723343515857],[1255,377,1723343515864],[1256,377,1723343515872],[1257,377,1723343515880],[1258,377,1723343515888],[1259,377,1723343515896],[1261,377,1723343515905],[1263,377,1723343515912],[1264,377,1723343515920],[1266,377,1723343515928],[1268,377,1723343515936],[1270,377,1723343515944],[1272,377,1723343515952],[1275,377,1723343515960],[1276,377,1723343515968],[1277,377,1723343515976],[1279,377,1723343515984],[1280,377,1723343516000],[1281,377,1723343516009],[1282,377,1723343516025],[1283,377,1723343516033],[1285,377,1723343516047],[1287,377,1723343516048],[1288,377,1723343516056],[1290,377,1723343516064],[1291,377,1723343516072],[1292,377,1723343516080],[1294,377,1723343516088],[1295,377,1723343516096],[1296,377,1723343516105],[1298,377,1723343516112],[1299,377,1723343516121],[1300,377,1723343516128],[1302,377,1723343516136],[1303,377,1723343516144],[1305,377,1723343516152],[1307,377,1723343516160],[1309,377,1723343516168],[1310,377,1723343516176],[1312,377,1723343516184],[1315,377,1723343516192],[1316,377,1723343516200],[1318,377,1723343516211],[1320,377,1723343516216],[1322,377,1723343516225],[1323,377,1723343516232],[1325,377,1723343516241],[1327,377,1723343516248],[1329,377,1723343516256],[1332,377,1723343516264],[1334,377,1723343516273],[1336,377,1723343516281],[1339,377,1723343516289],[1341,377,1723343516296],[1343,377,1723343516305],[1345,377,1723343516312],[1346,377,1723343516320],[1348,377,1723343516328],[1349,377,1723343516337],[1350,377,1723343516344],[1351,377,1723343516353],[1353,377,1723343516360],[1354,377,1723343516368],[1355,377,1723343516376],[1356,377,1723343516388],[1357,377,1723343516392],[1358,377,1723343516401],[1360,377,1723343516409],[1361,377,1723343516416],[1363,377,1723343516424],[1364,377,1723343516433],[1366,377,1723343516441],[1368,377,1723343516449],[1369,377,1723343516456],[1371,377,1723343516465],[1373,377,1723343516472],[1375,377,1723343516480],[1376,377,1723343516488],[1378,377,1723343516497],[1379,377,1723343516504],[1380,377,1723343516530],[1381,377,1723343516567],[1382,377,1723343516568],[1384,377,1723343516576],[1385,377,1723343516584],[1386,377,1723343516592],[1387,377,1723343516600],[1388,377,1723343516608],[1389,377,1723343516616],[1390,377,1723343516625],[1391,377,1723343516632],[1392,377,1723343516641],[1393,377,1723343516648],[1394,377,1723343516664],[1395,377,1723343516672],[1396,377,1723343516689],[1397,377,1723343517035],[1398,377,1723343517040],[1399,377,1723343517048],[1400,377,1723343517068]]roch=[[0, 414, 1723343562711], [0, 415, 1723343562719], [1, 415, 1723343562735], [2, 415, 1723343562751], [3, 415, 1723343562767], [4, 415, 1723343562783], [5, 415, 1723343562799], [6, 415, 1723343562808], [7, 415, 1723343562818], [8, 415, 1723343562831], [9, 415, 1723343562847], [10, 415, 1723343562855], [11, 414, 1723343562863], [12, 414, 1723343562871], [13, 414, 1723343562887], [13, 413, 1723343562894], [14, 413, 1723343562903], [15, 413, 1723343562975], [14, 413, 1723343563159], [13, 413, 1723343563175], [12, 413, 1723343563191], [11, 413, 1723343563223], [10, 413, 1723343563255], [9, 413, 1723343563263], [8, 413, 1723343563271], [7, 413, 1723343563278], [6, 413, 1723343563286], [5, 413, 1723343563295], [4, 413, 1723343563302], [3, 413, 1723343563311], [2, 413, 1723343563375], [1, 413, 1723343563490], [0, 413, 1723343563527], [0, 413, 1723343564062]]def get_track(distance): sliders=[] timenow=str(int(time.time()/10)) for i in track: if i[0] &lt;= distance+track[0][0]: sliders.append([i[0],i[1],int(timenow+str(i[2])[-4:])]) else: sliders.append([distance + track[0][0], i[1], int(timenow + str(i[2])[-4:]) + 700 + int(random.random() * 1000)]) break lastx=sliders[-1][0] lasty=sliders[-1][1] for i in range(len(roch)-1): if i == 0 and roch[i][0] == 0: times=sliders[-1][2] else: times=sliders[-1][2]+(roch[i+1][2]-roch[i][2]) sliders.append([lastx+roch[i][0],lasty,times]) sliders.append([lastx+roch[-1][0],lasty,sliders[-1][2]+1000]) return sliders track数组为从滑块起点到滑块末尾的轨迹 roch为晃动的轨迹 roch的录制：先打开滑块，准备手动滑一次，先从滑块开始然后滑正确的滑块缺口上稳住鼠标，然后模拟晃动往左或者往右进行滑动，然后再滑回滑块的位置 后面这一段就是晃动的轨迹了。把后面这一段的x减去滑块缺口的位置就可以得出0 1 2 3 4 5 ··· 5 4 3 2 1 0相关的轨迹点 得到两个数组后进行裁剪的合成，首先生成时间戳，利用track中的时间戳后4位+当前时间戳的前6位 组成新的时间戳，先从trach数组中获取滑块起点到滑块位置的轨迹点，例如当前滑块位置为103，则获取起点到103的轨迹点，然后再最后面添加一个精准的轨迹点 使用distance + track[0][0]的方法来添加最后一个轨迹点。 得到上面的正常轨迹之后，通过率也能有60%左右，还想通过率再高一点就在正常轨迹的后面拼接晃动的轨迹点。 在sliders后的第一位添加时 时间戳为正常轨迹的最后一个点的时间戳作为起始点，后续的每一个晃动轨迹点的时间戳都为前一项的时间戳+晃动数组中后一位的时间戳-当前晃动数组中的时间戳 times=sliders[-1][2]+(roch[i+1][2]-roch[i][2])。 全的代码就不放上了，还有一个点需要注意的获取验证码和提交之间不能太快，不然就都是失败，延迟大概5秒左右再提交。 最后附上被折磨了几天后的结果~","categories":[{"name":"Python","slug":"Python","permalink":"https://wantoper.github.io/categories/Python/"}],"tags":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"https://wantoper.github.io/tags/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"}]},{"title":"python通过Airscript API操作Wps 云表格","slug":"python通过Airscript-API操作Wps-云表格","date":"2024-09-06T06:43:56.000Z","updated":"2024-09-10T06:58:52.788Z","comments":true,"path":"2024/09/06/python通过Airscript-API操作Wps-云表格/","permalink":"https://wantoper.github.io/2024/09/06/python%E9%80%9A%E8%BF%87Airscript-API%E6%93%8D%E4%BD%9CWps-%E4%BA%91%E8%A1%A8%E6%A0%BC/","excerpt":"","text":"python通过Airscript API操作Wps 云表格 前言 最近的一个新需求需要读取Excel表格，但是表格又为云文档 不方便导出处理，思来想去，找了一些资料也没有响应的方案，有一种基于NodeJs的SDK方案被我放弃了 JSSDK 首先是调用改方法需要收费，其次需要安装Nodejs的环境较为麻烦，所以选择弃用该方案。采用了Airscrpt。 Airscript官方文档 Airscript 脚本 类似于Javascript 分为1.0版本和2.0版本 2.0版本更为兼容JS的一些API 创建一个Airscript脚本 创建完成后就可以编写所需的代码了好在官方提供了一个Ai模型，可以生成代码，加快了上手的速度，该服务还是蛮不错的。 给出一个自己写的demoApi 可以结合Python来远程调用 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960const sheet = ActiveSheetconst usedRange = sheet.UsedRangefunction alter_bgcolor_byvalue(param)&#123; let add= sheet.Range(param.column) let c = add.Find(param.value, undefined, xlValues) let isalter=false if (c != null) &#123; let firstAddress = c.Address() do &#123; c.Interior.Color = param.color c = add.FindNext(c) isalter=true &#125;while (c != null &amp;&amp; c.Address() != firstAddress) &#125; return isalter&#125;function exist_byvalue(param)&#123; let add = sheet.Range(param.column) let c = add.Find(param.value, undefined, xlValues) if (c != null) &#123; return true &#125;else&#123; return false &#125;&#125;function find_all_A_By_B(param)&#123; let order_arr=[] let add=sheet.Range(param.column) let c = add.Find(param.value, undefined, xlValues) if (c != null) &#123; let firstAddress = c.Address() do &#123; order_arr.push(c.Offset(param.row, param.cell).Value2) c = add.FindNext(c) &#125;while (c != null &amp;&amp; c.Address() != firstAddress) &#125; return order_arr&#125;// orders=find_all_A_By_B(param)// console.log(orders)switch(Context.argv.func)&#123; case &#x27;alter_bgcolor_byvalue&#x27;: return alter_bgcolor_byvalue(Context.argv.param) case &#x27;exist_byvalue&#x27;: return exist_byvalue(Context.argv.param) case &#x27;find_all_A_By_B&#x27;: // param=&#123; // &quot;value&quot;:&quot;2024年9月6日&quot;, // &quot;row&quot;:1, // &quot;cell&quot;:1, // &quot;column&quot;:&quot;D:D&quot; // &#125; return find_all_A_By_B(Context.argv.param) default: return &quot;未知function&quot;&#125; 获取API令牌和webhook调用链接点击这里可以获取脚本令牌，这个是账号级的可以调用其他任何脚本。 在此处获取脚本的webhook 每个脚本都有单独的一个webhook 其形式是http的url链接https://www.kdocs.cn/api/v3/ide/file/XXXXX/script/XXXX/sync_task Python调用对于相关使用可以参考官方的文档:脚本令牌 下方填入你的令牌，和webhook链接即可 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677from loguru import loggerimport requestsclass KColor(): #value = r + 256*g + 256*256*b Red = 255 #(255, 0, 0) Green = 65280 #(0, 255, 0) Blue = 16711680 #(0, 0, 255) Yellow = 65535 #(255, 255, 0) White = 16777215 #(255, 255, 255) Pink = 12434430 #(254, 187, 189)class KdocsApi: def __init__(self,token,url): self.url = url self.headers = &#123; &#x27;AirScript-Token&#x27;: token, &#x27;Content-Type&#x27;: &#x27;application/json&#x27; &#125; def run_script(self, func, param,sheet_name): json_data = &#123; &#x27;Context&#x27;: &#123; &#x27;argv&#x27;: &#123; &#x27;param&#x27;: param, &#x27;func&#x27;: func &#125;, &#x27;sheet_name&#x27;: sheet_name &#125;, &#125; response = requests.post(self.url, headers=self.headers, json=json_data) logger.info(f&quot;请求函数:&#123;func&#125; 请求结果：&#123;response.text&#125;&quot;) return response.json() # 根据值修改背景颜色 # value: 值 # column: 列 # color_value: 颜色值 def alter_bgcolor_byvalue(self,value,column,color_value,sheet_name): param = &#123; &#x27;value&#x27;: value, &#x27;column&#x27;: column, &#x27;color&#x27;: color_value &#125; return self.run_script(&#x27;alter_bgcolor_byvalue&#x27;,param,sheet_name) # 判断是否存在 # value: 值 # column: 列 # sheet_name: 表名 # exis=ks.exist_byvalue(&#x27;2276836611175683557&#x27;,&#x27;E:E&#x27;,sheet_name) def exist_byvalue(self,value,column,sheet_name): parma=&#123; &#x27;value&#x27;: value, &#x27;column&#x27;: column &#125; return self.run_script(&#x27;exist_byvalue&#x27;,parma,sheet_name) # 根据B列的值查找偏移后A列的值 # value: B列的值 # row: 偏移行数 # cell: 偏移列数 # column: B列的范围 # sheet_name: 表名 def find_all_A_By_B(self,value,row,cell,column,sheet_name): param = &#123; &quot;value&quot;:value, &quot;row&quot;: row, &quot;cell&quot;: cell, &quot;column&quot;: column &#125; return self.run_script(&#x27;find_all_A_By_B&#x27;,param,sheet_name)[&#x27;data&#x27;][&#x27;result&#x27;]if __name__ == &#x27;__main__&#x27;: api=KdocsApi(&#x27;脚本令牌&#x27;,&#x27;webhook&#x27;) api.alter_bgcolor_byvalue(&#x27;ABCDEF&#x27;,&#x27;E:E&#x27;,KColor.Pink,&#x27;Sheet1&#x27;)","categories":[{"name":"Python","slug":"Python","permalink":"https://wantoper.github.io/categories/Python/"}],"tags":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"https://wantoper.github.io/tags/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"}]},{"title":"新Blog","slug":"新Blog","date":"2024-09-06T01:29:08.000Z","updated":"2024-09-11T00:56:24.696Z","comments":true,"path":"2024/09/06/新Blog/","permalink":"https://wantoper.github.io/2024/09/06/%E6%96%B0Blog/","excerpt":"","text":"第一次尝试使用Hexo来编写博客，算是搬了个家！","categories":[],"tags":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"https://wantoper.github.io/tags/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"}]}],"categories":[{"name":"Python","slug":"Python","permalink":"https://wantoper.github.io/categories/Python/"}],"tags":[{"name":"杂七杂八","slug":"杂七杂八","permalink":"https://wantoper.github.io/tags/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"}]}