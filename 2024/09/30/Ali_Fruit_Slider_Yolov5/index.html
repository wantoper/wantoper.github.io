<!DOCTYPE html><html lang="cn"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><link rel="shortcut icon" href="/assets/imgs/favicon.ico" type="image/x-icon"><title>某里两种版本的水果滑块训练——Yolov5 | Wantoper</title><meta name="keywords" content="杂七杂八"><meta name="keywords" content=""><link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"><link rel="stylesheet" href="https://at.alicdn.com/t/c/font_3435803_wadmwlqwhps.css"><link rel="stylesheet" href="/style/index.css"><link rel="stylesheet" href="/assets/lib/prettify/prettify.css"><link rel="stylesheet" href="/assets/lib/fancybox/fancybox.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="app"><header class="header yu13_p_30" id="header"><div class="header_left"><h1 class="yu13_logo"><a id="logo" href="/">Wantoper</a></h1></div><div class="header_right"><div class="header_search yu13_cursor yu13_hover" id="search"><i class="yufont icon-sousuo"></i></div><div class="line yu13_p_10"></div><nav class="header_nav"><ul class="nav"><li class="menu_nav"><a href="/"><i class="fas"></i><span> 首页</span></a></li><li class="menu_nav"><a href="/archives"><i class="fas"></i><span> 归档</span></a></li><li class="menu_nav"><a href="/categorys"><i class="fas"></i><span> 分类</span></a></li><li class="menu_nav"><a href="/tags"><i class="fas"></i><span> 标签</span></a></li><li class="menu_nav"><a href="/links"><i class="fas"></i><span> 友情链接</span></a></li></ul></nav></div></header><div class="article_topbg" style="background-image: url('/assets/imgs/top_bg.jpg')"><div class="topbg_container"><h1 class="title">某里两种版本的水果滑块训练——Yolov5</h1><div class="info"><div class="article_meta"><div class="left"><span class="avatar"><img class="yu13_img_rotate" src="/assets/imgs/avatar.jpg" alt=""/><a href="/about">Wantoper</a></span><span><i class="yufont icon-yingyong"></i><a href="/categories/Python/">Python</a></span><span><i class="yufont icon-rili"></i>2024-9-30</span></div></div></div></div></div><div class="container log_container clearfix"><div class="article_content left"> <div class="article_main"><article class="content"><div class="content_text article_text"> <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><strong>本章不涉及任何逆向</strong>  仅对训练过程及识别方式进行讨论</p>
<p>总所周知 某里系有个著名的滑块 227 由于技术且未达到 所以先不搓js层次的，先对识别部分的先来练练手。某大佬说因为早期验证码内都是一些水果 所以就称为<strong>水果滑块</strong>。</p>
<h1 id="验证码字体处的识别"><a href="#验证码字体处的识别" class="headerlink" title="验证码字体处的识别"></a>验证码字体处的识别</h1><p>目前我所知的有两个版本</p>
<p>1.0最早期的比较简单在风控后就会出现 一般在登录可以见到 227多滑几次之后就会出现</p>
<p>1.0的识别相对简单可以直接降噪后用opencv识别 但是这里有点坑 直接用opencv读会很模糊后面会提到</p>
<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930144822078.png"  style="width:50%;" />



<p>2.0的目前没见到过 但是有一个接口会直接访问<a target="_blank" rel="noopener" href="https://scportal.taobao.com/quali_show.htm">D</a></p>
<p>2.0的字体识别位置改为了两张图片反复切换，利用了视觉停留，看清了图片内容，想用截屏的方式是行不通的，两张图片都是带噪点的 单独拎出来是看不清的。</p>
<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930144157318.png"  style="width:50%;" />

<p>在接口处可以看到两张图片</p>
<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930144453729.png"  style="width:50%;" />



<p>两种方式采用opencv处理后用DDDDOCR进行识别 效果完全够用！</p>
<h2 id="1-0的文字处理"><a href="#1-0的文字处理" class="headerlink" title="1.0的文字处理"></a>1.0的文字处理</h2><p>把坑说在前头</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">opencv_img</span>(<span class="params">base64_str</span>):</span><br><span class="line">    img_data = base64.b64decode(base64_str.split(<span class="string">&#x27;,&#x27;</span>)[<span class="number">1</span>])</span><br><span class="line">    nparr = np.frombuffer(img_data, np.uint8)</span><br><span class="line">    img=cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> img.shape[<span class="number">2</span>] == <span class="number">4</span>:</span><br><span class="line">        b, g, r, a = cv2.split(img)</span><br><span class="line">        white_background = np.ones((img.shape[<span class="number">0</span>], img.shape[<span class="number">1</span>], <span class="number">3</span>), dtype=np.uint8) * <span class="number">255</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            white_background[:, :, c] = b * (a / <span class="number">255.0</span>) + white_background[:, :, c] * (<span class="number">1</span> - a / <span class="number">255.0</span>)</span><br><span class="line">        cv2.imshow(<span class="string">&#x27;Image with Transparency&#x27;</span>, white_background)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        cv2.imshow(<span class="string">&#x27;Image&#x27;</span>, img)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure>

<p>一开始我用这种方式去读取base64的背景图 发现效果是这样的</p>
<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930145501750.png"  style="width:50%;" />

<p>非常糊，人眼都不太好看清更别说ocr了</p>
<p>后面琢磨了一天始终没发现是读取图片代码的问题以为是opencv的处理问题，最后挨个试才发现这边需要保留透明通道 改用下面的代码后就行了 很高清<del>无码</del>！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imdecode(nparr, cv2.IMREAD_UNCHANGED)</span><br></pre></td></tr></table></figure>

<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930145913727.png" class="" title="image-20240930145913727">

<p>这样再丢给ocr就嘚嘚得了</p>
<p>最后因为我需要打包成exe使用 为了压缩体积所以选择不使用Opencv 采用了PIL库</p>
<p>需要用Opencv来识别的可以丢给AI帮你生成一下</p>
<p>附上PIL的代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def deal_que_new_img(bin_image):</span><br><span class="line">	ocr = DdddOcr(show_ad=False)</span><br><span class="line">    img = Image.open(BytesIO(bin_image)).convert(&quot;RGBA&quot;)</span><br><span class="line">    background = Image.new(&quot;RGBA&quot;, img.size, (255, 255, 255, 255))</span><br><span class="line">    img = Image.alpha_composite(background, img)</span><br><span class="line">    image = img.crop((143, 0, img.size[0], img.size[1]))</span><br><span class="line">    # image.show()</span><br><span class="line">    ocr_res = ocr.classification(image).split(&#x27;后&#x27;)[0]</span><br><span class="line">    logger.info(f&quot;识别结果：&#123;ocr_res&#125;&quot;)</span><br><span class="line">    return ocr_res</span><br><span class="line">    </span><br><span class="line">titleimg=&quot;base64&quot;</span><br><span class="line">que_img = b64decode(titleimg.split(&#x27;base64,&#x27;)[-1])</span><br><span class="line">queue = deal_que_new_img(que_img)</span><br><span class="line"> </span><br></pre></td></tr></table></figure>

<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930150946480.png" class="" title="image-20240930150946480">

<p>至此第一步就完成了</p>
<h2 id="2-0的文字处理"><a href="#2-0的文字处理" class="headerlink" title="2.0的文字处理"></a>2.0的文字处理</h2><p>2.0的较为复杂一些一开始也思考了挺久，都准备上模型了，后面在公众号找到一个大佬的文章看了之后茅<del>厕</del>顿开 醍醐灌顶。</p>
<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930152017397.png" class="" title="image-20240930152017397">

<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930152027210.png" class="" title="image-20240930152027210">

<p>既然利用了视觉停留 在页面上图片反复跳转时字体还是蛮清晰的，所以将两张图片合二为一进行像素点遍历，保留两张图都存在的像素点，一张有 一张没有的像素点就去除掉。没有的话设为0白色 都有的话设为255黑色</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#同时遍历两张图片，找到不同的像素点 删除掉</span></span><br><span class="line"><span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(original.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(original.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> np.<span class="built_in">any</span>(original[y, x]) == <span class="number">0</span> <span class="keyword">and</span> np.<span class="built_in">any</span>(original2[y, x]) == <span class="number">0</span>:</span><br><span class="line">            original[y, x] = <span class="number">0</span></span><br><span class="line">            original2[y, x] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            original[y, x] = <span class="number">255</span></span><br><span class="line">            original2[y, x] = <span class="number">255</span></span><br></pre></td></tr></table></figure>

<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930153200868.png"  style="width:50%;" />

<p>效果还是挺好的 从”后”进行切割一下就行</p>
<h1 id="滑块图的处理及训练"><a href="#滑块图的处理及训练" class="headerlink" title="滑块图的处理及训练"></a>滑块图的处理及训练</h1><p>图片长这样</p>
<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930153630277.png"  style="width:50%;" />

<p>多刷了几次我发现这样有规律 一般需要验证的物体的都在最后一个 例如两个熊猫 三个苹松鼠 一个鱼 只需要找到最后一个物体的坐标即可</p>
<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930154040381.png" class="">

<p>有了这个规律就不需要语义模型来二次识别了</p>
<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930154141597.png" class="" title="image-20240930154141597">

<p>后期训练出yolo后遍历x坐标最大的即可获得出坐标位置。</p>
<h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><p>训练前先要获取数据主要是图片和类别</p>
<p>获取直接就猛猛调接口就行了 也不建议太快 这里附上代码图 代码就不贴了</p>
<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930163529811.png"  style="width:50%;" />

<p>调接口把背景图保存，然后用前面的方法识别一下标题上的字获取类别，下来230多张就已经足够了 97%的准确率。类别的话可以多跑一些 基本上就能全跑完</p>
<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930163657101.png"  style="width:50%;" />

<h2 id="打标注"><a href="#打标注" class="headerlink" title="打标注"></a>打标注</h2><p>打标注我这里用label-studio+sam半自动标注 只需要点点点就行 可以看之前发过的一篇</p>
<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240912164230577.png"  style="width:50%;" />

<p>我这里只标注了230张就用来训练了</p>
<p>最后导出成yolo格式的数据</p>
<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930164217157.png"  style="width:50%;" />

<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930164259453.png"  style="width:50%;" />

<h2 id="Yolov5的训练"><a href="#Yolov5的训练" class="headerlink" title="Yolov5的训练"></a>Yolov5的训练</h2><p>预装Yolov5 Copy一份coco的修改成自己的yaml</p>
<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930164519668.png"  style="width:50%;" />



<p>简单修改一下train.py 的参数就可以运行 开始训练了 我这里只改了数据集的位置 其他都用默认的就好了</p>
<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930164745644.png"  style="width:50%;" />

<p>训练好后测试一下 识别效果还不错</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> loguru <span class="keyword">import</span> logger</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model=torch.hub.load(<span class="string">&quot;.&quot;</span>, <span class="string">&quot;custom&quot;</span>, path=<span class="string">&quot;./runs/train/exp12/weights/best.pt&quot;</span>, source=<span class="string">&quot;local&quot;</span>)</span><br><span class="line"></span><br><span class="line">img=<span class="string">r&#x27;./testdetect.png&#x27;</span></span><br><span class="line">reslut=model(img)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(reslut))</span><br><span class="line"><span class="comment">#输出结果 类别，置信度，坐标</span></span><br><span class="line">logger.info(reslut.pandas().xyxy[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">reslut.show()</span><br></pre></td></tr></table></figure>



<img src="/2024/09/30/Ali_Fruit_Slider_Yolov5/image-20240930165313552.png" class="" title="image-20240930165313552">

<p>按照前面的规律就可以直接用了</p>
<p>1.使用获取标题 </p>
<p>2.调用模型获得图片中所有分类</p>
<p>3.在模型结果中进行遍历 获取标题类别的所有坐标</p>
<p>4.遍历坐标x最大位置坐标就是了 最后还要加上当前框框的宽度就行了</p>
<h2 id="迁移部署"><a href="#迁移部署" class="headerlink" title="迁移部署"></a>迁移部署</h2><p>因为考虑了部署机器性能的原因我对其进行了迁移</p>
<h3 id="转为Onnx"><a href="#转为Onnx" class="headerlink" title="转为Onnx"></a>转为Onnx</h3><p>直接在Yolo项目下对模型转换成onnx格式的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> models.experimental <span class="keyword">import</span> attempt_load</span><br><span class="line"><span class="comment"># 加载模型权重</span></span><br><span class="line">model = attempt_load(<span class="string">&#x27;runs/train/exp12/weights/best.pt&#x27;</span>, map_location=torch.device(<span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line"><span class="comment"># 设置模型为评估模式</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># 准备一个示例输入</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">640</span>, <span class="number">640</span>)  <span class="comment"># 假设输入图像大小为 640x640</span></span><br><span class="line"><span class="comment"># 导出模型</span></span><br><span class="line"><span class="comment">#Lib\site-packages\torch\nn\modules\ activation.py</span></span><br><span class="line">torch.onnx.export(model, input_tensor, <span class="string">&#x27;AliFruit.onnx&#x27;</span>,opset_version=<span class="number">11</span>)</span><br></pre></td></tr></table></figure>

<p>转为后封装一下模型 还有一些必要的函数nms xywh2xyxy extrack letterbox这些我都是从yolo里面Copy出来的 为了最简化 这里贴出一下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line">class YOLOV5_ONNX(object):</span><br><span class="line">    def __init__(self,onnx_path):</span><br><span class="line">        &#x27;&#x27;&#x27;初始化onnx&#x27;&#x27;&#x27;</span><br><span class="line">        self.onnx_session=onnxruntime.InferenceSession(onnx_path)</span><br><span class="line">        self.classes=[&#x27;乌龟&#x27;,&#x27;企鹅&#x27;,&#x27;伞&#x27;,&#x27;免子&#x27;,&#x27;冰激凌&#x27;,&#x27;凤梨&#x27;,&#x27;包&#x27;,&#x27;南瓜&#x27;,&#x27;吉他&#x27;,&#x27;大象&#x27;,&#x27;太阳花&#x27;,&#x27;宇航员&#x27;,&#x27;帐蓬&#x27;,&#x27;帽子&#x27;,&#x27;房子&#x27;,&#x27;挂锁&#x27;,&#x27;杯子&#x27;,&#x27;松鼠&#x27;,&#x27;枕头&#x27;,&#x27;树&#x27;,&#x27;树袋熊&#x27;,&#x27;椅子&#x27;,&#x27;气球&#x27;,&#x27;汉堡包&#x27;,&#x27;熊猫&#x27;,&#x27;玫瑰花&#x27;,&#x27;瓢虫&#x27;,&#x27;瓶子&#x27;,&#x27;皇冠&#x27;,&#x27;篮子&#x27;,&#x27;耳机&#x27;,&#x27;花盆&#x27;,&#x27;苹果&#x27;,&#x27;草莓&#x27;,&#x27;蘑菇&#x27;,&#x27;蛋糕&#x27;,&#x27;蝴蝶&#x27;,&#x27;裙子&#x27;,&#x27;足球&#x27;,&#x27;车&#x27;,&#x27;轮胎&#x27;,&#x27;铲土机&#x27;,&#x27;闹钟&#x27;,&#x27;鞋&#x27;,&#x27;马&#x27;,&#x27;鱼&#x27;,&#x27;鸟&#x27;,&#x27;鸭子&#x27;]</span><br><span class="line">    def letterbox(self,img, new_shape=(640, 640), color=(114, 114, 114), auto=False, scaleFill=False, scaleup=True,stride=32):</span><br><span class="line">        &#x27;&#x27;&#x27;图片归一化&#x27;&#x27;&#x27;</span><br><span class="line">        # Resize and pad image while meeting stride-multiple constraints</span><br><span class="line">        shape = img.shape[:2]  # current shape [height, width]</span><br><span class="line">        if isinstance(new_shape, int):</span><br><span class="line">            new_shape = (new_shape, new_shape)</span><br><span class="line"></span><br><span class="line">        # Scale ratio (new / old)</span><br><span class="line">        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])</span><br><span class="line">        if not scaleup:  # only scale down, do not scale up (for better test mAP)</span><br><span class="line">            r = min(r, 1.0)</span><br><span class="line"></span><br><span class="line">        # Compute padding</span><br><span class="line">        ratio = r, r  # width, height ratios</span><br><span class="line"></span><br><span class="line">        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))</span><br><span class="line">        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding</span><br><span class="line"></span><br><span class="line">        if auto:  # minimum rectangle</span><br><span class="line">            dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding</span><br><span class="line">        elif scaleFill:  # stretch</span><br><span class="line">            dw, dh = 0.0, 0.0</span><br><span class="line">            new_unpad = (new_shape[1], new_shape[0])</span><br><span class="line">            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios</span><br><span class="line"></span><br><span class="line">        dw /= 2  # divide padding into 2 sides</span><br><span class="line">        dh /= 2</span><br><span class="line"></span><br><span class="line">        if shape[::-1] != new_unpad:  # resize</span><br><span class="line">            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)</span><br><span class="line"></span><br><span class="line">        top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))</span><br><span class="line">        left, right = int(round(dw - 0.1)), int(round(dw + 0.1))</span><br><span class="line"></span><br><span class="line">        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border</span><br><span class="line">        return img, ratio, (dw, dh)</span><br><span class="line">    def infer(self,src_img):</span><br><span class="line">        &#x27;&#x27;&#x27;执行前向操作预测输出&#x27;&#x27;&#x27;</span><br><span class="line">        or_img = self.letterbox(src_img, (640, 640), stride=32)[0]</span><br><span class="line">        # BGR2RGB</span><br><span class="line">        img = or_img[:, :, ::-1].transpose(2, 0, 1)  # BGR2RGB和HWC2CHW</span><br><span class="line">        img = img.astype(dtype=np.float32)</span><br><span class="line">        img /= 255.0</span><br><span class="line">        img = np.expand_dims(img, axis=0)</span><br><span class="line">        pred = self.onnx_session.run(None, &#123;self.onnx_session.get_inputs()[0].name: img&#125;)[0]</span><br><span class="line"></span><br><span class="line">        outbox = model.extrack(pred, 0.5, 0.5)</span><br><span class="line"></span><br><span class="line">        # draw(or_img, outbox)</span><br><span class="line">        # cv2.imshow(&#x27;result&#x27;, or_img)</span><br><span class="line">        # cv2.waitKey(0)</span><br><span class="line"></span><br><span class="line">        return outbox</span><br><span class="line"></span><br><span class="line">        # dets:  array [x,6] 6个值分别为x1,y1,x2,y2,score,class</span><br><span class="line">        # thresh: 阈值</span><br><span class="line">    def nms(self, dets, thresh):</span><br><span class="line">        x1 = dets[:, 0]</span><br><span class="line">        y1 = dets[:, 1]</span><br><span class="line">        x2 = dets[:, 2]</span><br><span class="line">        y2 = dets[:, 3]</span><br><span class="line">        # -------------------------------------------------------</span><br><span class="line">        #   计算框的面积</span><br><span class="line">        #	置信度从大到小排序</span><br><span class="line">        # -------------------------------------------------------</span><br><span class="line">        areas = (y2 - y1 + 1) * (x2 - x1 + 1)  # 公式=长*宽</span><br><span class="line">        scores = dets[:, 4]</span><br><span class="line">        keep = []</span><br><span class="line">        index = scores.argsort()[::-1]</span><br><span class="line">        while index.size &gt; 0:</span><br><span class="line">            i = index[0]</span><br><span class="line">            keep.append(i)</span><br><span class="line">            # -------------------------------------------------------</span><br><span class="line">            #   计算相交面积</span><br><span class="line">            #	1.相交</span><br><span class="line">            #	2.不相交</span><br><span class="line">            # -------------------------------------------------------</span><br><span class="line"></span><br><span class="line">            x11 = np.maximum(x1[i], x1[index[1:]])</span><br><span class="line">            y11 = np.maximum(y1[i], y1[index[1:]])</span><br><span class="line">            x22 = np.minimum(x2[i], x2[index[1:]])</span><br><span class="line">            y22 = np.minimum(y2[i], y2[index[1:]])</span><br><span class="line"></span><br><span class="line">            w = np.maximum(0, x22 - x11 + 1)</span><br><span class="line">            h = np.maximum(0, y22 - y11 + 1)</span><br><span class="line"></span><br><span class="line">            overlaps = w * h</span><br><span class="line">            # -------------------------------------------------------</span><br><span class="line">            #   计算该框与其它框的IOU，去除掉重复的框，即IOU值大的框</span><br><span class="line">            #	IOU小于thresh的框保留下来</span><br><span class="line">            # -------------------------------------------------------</span><br><span class="line">            ious = overlaps / (areas[i] + areas[index[1:]] - overlaps)</span><br><span class="line">            idx = np.where(ious &lt;= thresh)[0]</span><br><span class="line">            index = index[idx + 1]</span><br><span class="line">        return keep</span><br><span class="line">    def xywh2xyxy(self, x):</span><br><span class="line">        # [x, y, w, h] to [x1, y1, x2, y2]</span><br><span class="line">        y = np.copy(x)</span><br><span class="line">        y[:, 0] = x[:, 0] - x[:, 2] / 2  # x=x-w/2</span><br><span class="line">        y[:, 1] = x[:, 1] - x[:, 3] / 2  # y=y-h/2</span><br><span class="line">        y[:, 2] = x[:, 0] + x[:, 2] / 2  # x=x+w/2</span><br><span class="line">        y[:, 3] = x[:, 1] + x[:, 3] / 2  # y=y+h/2</span><br><span class="line">        return y</span><br><span class="line">    def extrack(self, output, conf_thres=0.5, iou_thres=0.5):</span><br><span class="line">        output = np.squeeze(output)</span><br><span class="line">        # 过滤掉置信度小于0.5的框</span><br><span class="line">        outputcheck = output[..., 4] &gt; conf_thres</span><br><span class="line">        output = output[outputcheck]</span><br><span class="line"></span><br><span class="line">        # 获取每个框最大置信度的类别 放到第6列  x,y,w,h,conf,class·····</span><br><span class="line">        for i in range(len(output)):</span><br><span class="line">            output[i][5] = np.argmax(output[i][5:])</span><br><span class="line">        # 只取前6列 x,y,w,h,conf,class</span><br><span class="line">        output = output[..., 0:6]</span><br><span class="line">        # 将x,y,w,h转换为x1,y1,x2,y2</span><br><span class="line">        output = self.xywh2xyxy(output)</span><br><span class="line">        # 过滤掉重复的框</span><br><span class="line">        output1 = self.nms(output, iou_thres)</span><br><span class="line">        outputlist = []</span><br><span class="line">        for i in output1:</span><br><span class="line">            outputlist.append(output[i])</span><br><span class="line">        outputlist = np.array(outputlist)</span><br><span class="line">        return outputlist</span><br><span class="line">        </span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    model = YOLOV5_ONNX(onnx_path=&quot;./AliFruit.onnx&quot;)</span><br><span class="line">    background_img=&quot;base64&quot;</span><br><span class="line">    back_img = b64decode(background_img.split(&#x27;base64,&#x27;)[-1])</span><br><span class="line">    back_img = cv2.imdecode(np.frombuffer(back_img, np.uint8), cv2.IMREAD_COLOR)</span><br><span class="line">    result = model.infer(back_img).tolist()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>最后我使用了flask来部署调用</p>
<p>这里附上全部代码（1.0版本的）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br></pre></td><td class="code"><pre><span class="line">#encoding:utf-8</span><br><span class="line">import time</span><br><span class="line">from io import BytesIO</span><br><span class="line">import onnxruntime</span><br><span class="line">from loguru import logger</span><br><span class="line">from base64 import b64decode</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2</span><br><span class="line">from PIL import Image</span><br><span class="line">from ddddocr import DdddOcr</span><br><span class="line">from flask import Flask, request</span><br><span class="line"></span><br><span class="line">logger.add(&quot;Flask_Web.log&quot;, rotation=&quot;10 MB&quot;, encoding=&quot;utf-8&quot;, level=&quot;INFO&quot;)</span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line">class YOLOV5_ONNX(object):</span><br><span class="line">    def __init__(self,onnx_path):</span><br><span class="line">        &#x27;&#x27;&#x27;初始化onnx&#x27;&#x27;&#x27;</span><br><span class="line">        self.onnx_session=onnxruntime.InferenceSession(onnx_path)</span><br><span class="line">        self.classes=[&#x27;乌龟&#x27;,&#x27;企鹅&#x27;,&#x27;伞&#x27;,&#x27;免子&#x27;,&#x27;冰激凌&#x27;,&#x27;凤梨&#x27;,&#x27;包&#x27;,&#x27;南瓜&#x27;,&#x27;吉他&#x27;,&#x27;大象&#x27;,&#x27;太阳花&#x27;,&#x27;宇航员&#x27;,&#x27;帐蓬&#x27;,&#x27;帽子&#x27;,&#x27;房子&#x27;,&#x27;挂锁&#x27;,&#x27;杯子&#x27;,&#x27;松鼠&#x27;,&#x27;枕头&#x27;,&#x27;树&#x27;,&#x27;树袋熊&#x27;,&#x27;椅子&#x27;,&#x27;气球&#x27;,&#x27;汉堡包&#x27;,&#x27;熊猫&#x27;,&#x27;玫瑰花&#x27;,&#x27;瓢虫&#x27;,&#x27;瓶子&#x27;,&#x27;皇冠&#x27;,&#x27;篮子&#x27;,&#x27;耳机&#x27;,&#x27;花盆&#x27;,&#x27;苹果&#x27;,&#x27;草莓&#x27;,&#x27;蘑菇&#x27;,&#x27;蛋糕&#x27;,&#x27;蝴蝶&#x27;,&#x27;裙子&#x27;,&#x27;足球&#x27;,&#x27;车&#x27;,&#x27;轮胎&#x27;,&#x27;铲土机&#x27;,&#x27;闹钟&#x27;,&#x27;鞋&#x27;,&#x27;马&#x27;,&#x27;鱼&#x27;,&#x27;鸟&#x27;,&#x27;鸭子&#x27;]</span><br><span class="line">    def letterbox(self,img, new_shape=(640, 640), color=(114, 114, 114), auto=False, scaleFill=False, scaleup=True,stride=32):</span><br><span class="line">        &#x27;&#x27;&#x27;图片归一化&#x27;&#x27;&#x27;</span><br><span class="line">        # Resize and pad image while meeting stride-multiple constraints</span><br><span class="line">        shape = img.shape[:2]  # current shape [height, width]</span><br><span class="line">        if isinstance(new_shape, int):</span><br><span class="line">            new_shape = (new_shape, new_shape)</span><br><span class="line"></span><br><span class="line">        # Scale ratio (new / old)</span><br><span class="line">        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])</span><br><span class="line">        if not scaleup:  # only scale down, do not scale up (for better test mAP)</span><br><span class="line">            r = min(r, 1.0)</span><br><span class="line"></span><br><span class="line">        # Compute padding</span><br><span class="line">        ratio = r, r  # width, height ratios</span><br><span class="line"></span><br><span class="line">        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))</span><br><span class="line">        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding</span><br><span class="line"></span><br><span class="line">        if auto:  # minimum rectangle</span><br><span class="line">            dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding</span><br><span class="line">        elif scaleFill:  # stretch</span><br><span class="line">            dw, dh = 0.0, 0.0</span><br><span class="line">            new_unpad = (new_shape[1], new_shape[0])</span><br><span class="line">            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios</span><br><span class="line"></span><br><span class="line">        dw /= 2  # divide padding into 2 sides</span><br><span class="line">        dh /= 2</span><br><span class="line"></span><br><span class="line">        if shape[::-1] != new_unpad:  # resize</span><br><span class="line">            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)</span><br><span class="line"></span><br><span class="line">        top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))</span><br><span class="line">        left, right = int(round(dw - 0.1)), int(round(dw + 0.1))</span><br><span class="line"></span><br><span class="line">        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border</span><br><span class="line">        return img, ratio, (dw, dh)</span><br><span class="line">    def infer(self,src_img):</span><br><span class="line">        &#x27;&#x27;&#x27;执行前向操作预测输出&#x27;&#x27;&#x27;</span><br><span class="line">        or_img = self.letterbox(src_img, (640, 640), stride=32)[0]</span><br><span class="line">        # BGR2RGB</span><br><span class="line">        img = or_img[:, :, ::-1].transpose(2, 0, 1)  # BGR2RGB和HWC2CHW</span><br><span class="line">        img = img.astype(dtype=np.float32)</span><br><span class="line">        img /= 255.0</span><br><span class="line">        img = np.expand_dims(img, axis=0)</span><br><span class="line">        pred = self.onnx_session.run(None, &#123;self.onnx_session.get_inputs()[0].name: img&#125;)[0]</span><br><span class="line"></span><br><span class="line">        outbox = model.extrack(pred, 0.5, 0.5)</span><br><span class="line"></span><br><span class="line">        # draw(or_img, outbox)</span><br><span class="line">        # cv2.imshow(&#x27;result&#x27;, or_img)</span><br><span class="line">        # cv2.waitKey(0)</span><br><span class="line"></span><br><span class="line">        return outbox</span><br><span class="line"></span><br><span class="line">        # dets:  array [x,6] 6个值分别为x1,y1,x2,y2,score,class</span><br><span class="line">        # thresh: 阈值</span><br><span class="line">    def nms(self, dets, thresh):</span><br><span class="line">        x1 = dets[:, 0]</span><br><span class="line">        y1 = dets[:, 1]</span><br><span class="line">        x2 = dets[:, 2]</span><br><span class="line">        y2 = dets[:, 3]</span><br><span class="line">        # -------------------------------------------------------</span><br><span class="line">        #   计算框的面积</span><br><span class="line">        #	置信度从大到小排序</span><br><span class="line">        # -------------------------------------------------------</span><br><span class="line">        areas = (y2 - y1 + 1) * (x2 - x1 + 1)  # 公式=长*宽</span><br><span class="line">        scores = dets[:, 4]</span><br><span class="line">        keep = []</span><br><span class="line">        index = scores.argsort()[::-1]</span><br><span class="line">        while index.size &gt; 0:</span><br><span class="line">            i = index[0]</span><br><span class="line">            keep.append(i)</span><br><span class="line">            # -------------------------------------------------------</span><br><span class="line">            #   计算相交面积</span><br><span class="line">            #	1.相交</span><br><span class="line">            #	2.不相交</span><br><span class="line">            # -------------------------------------------------------</span><br><span class="line"></span><br><span class="line">            x11 = np.maximum(x1[i], x1[index[1:]])</span><br><span class="line">            y11 = np.maximum(y1[i], y1[index[1:]])</span><br><span class="line">            x22 = np.minimum(x2[i], x2[index[1:]])</span><br><span class="line">            y22 = np.minimum(y2[i], y2[index[1:]])</span><br><span class="line"></span><br><span class="line">            w = np.maximum(0, x22 - x11 + 1)</span><br><span class="line">            h = np.maximum(0, y22 - y11 + 1)</span><br><span class="line"></span><br><span class="line">            overlaps = w * h</span><br><span class="line">            # -------------------------------------------------------</span><br><span class="line">            #   计算该框与其它框的IOU，去除掉重复的框，即IOU值大的框</span><br><span class="line">            #	IOU小于thresh的框保留下来</span><br><span class="line">            # -------------------------------------------------------</span><br><span class="line">            ious = overlaps / (areas[i] + areas[index[1:]] - overlaps)</span><br><span class="line">            idx = np.where(ious &lt;= thresh)[0]</span><br><span class="line">            index = index[idx + 1]</span><br><span class="line">        return keep</span><br><span class="line">    def xywh2xyxy(self, x):</span><br><span class="line">        # [x, y, w, h] to [x1, y1, x2, y2]</span><br><span class="line">        y = np.copy(x)</span><br><span class="line">        y[:, 0] = x[:, 0] - x[:, 2] / 2  # x=x-w/2</span><br><span class="line">        y[:, 1] = x[:, 1] - x[:, 3] / 2  # y=y-h/2</span><br><span class="line">        y[:, 2] = x[:, 0] + x[:, 2] / 2  # x=x+w/2</span><br><span class="line">        y[:, 3] = x[:, 1] + x[:, 3] / 2  # y=y+h/2</span><br><span class="line">        return y</span><br><span class="line">    def extrack(self, output, conf_thres=0.5, iou_thres=0.5):</span><br><span class="line">        output = np.squeeze(output)</span><br><span class="line">        # 过滤掉置信度小于0.5的框</span><br><span class="line">        outputcheck = output[..., 4] &gt; conf_thres</span><br><span class="line">        output = output[outputcheck]</span><br><span class="line"></span><br><span class="line">        # 获取每个框最大置信度的类别 放到第6列  x,y,w,h,conf,class·····</span><br><span class="line">        for i in range(len(output)):</span><br><span class="line">            output[i][5] = np.argmax(output[i][5:])</span><br><span class="line">        # 只取前6列 x,y,w,h,conf,class</span><br><span class="line">        output = output[..., 0:6]</span><br><span class="line">        # 将x,y,w,h转换为x1,y1,x2,y2</span><br><span class="line">        output = self.xywh2xyxy(output)</span><br><span class="line">        # 过滤掉重复的框</span><br><span class="line">        output1 = self.nms(output, iou_thres)</span><br><span class="line">        outputlist = []</span><br><span class="line">        for i in output1:</span><br><span class="line">            outputlist.append(output[i])</span><br><span class="line">        outputlist = np.array(outputlist)</span><br><span class="line">        return outputlist</span><br><span class="line">def deal_que_new_img(bin_image):</span><br><span class="line">    img = Image.open(BytesIO(bin_image)).convert(&quot;RGBA&quot;)</span><br><span class="line">    background = Image.new(&quot;RGBA&quot;, img.size, (255, 255, 255, 255))</span><br><span class="line">    img = Image.alpha_composite(background, img)</span><br><span class="line">    image = img.crop((143, 0, img.size[0], img.size[1]))</span><br><span class="line">    # image.show()</span><br><span class="line">    ocr_res = ocr.classification(image).split(&#x27;后&#x27;)[0]</span><br><span class="line">    logger.info(f&quot;识别结果：&#123;ocr_res&#125;&quot;)</span><br><span class="line">    return ocr_res</span><br><span class="line"></span><br><span class="line">def draw(image, box_data):</span><br><span class="line">    # -------------------------------------------------------</span><br><span class="line">    #	取整，方便画框</span><br><span class="line">    # -------------------------------------------------------</span><br><span class="line">    boxes = box_data[..., :4].astype(np.int32)</span><br><span class="line">    scores = box_data[..., 4]</span><br><span class="line">    # print(scores)</span><br><span class="line">    classes = box_data[..., 5].astype(np.int32)</span><br><span class="line">    for box, score, cl in zip(boxes, scores, classes):</span><br><span class="line">        top, left, right, bottom = box</span><br><span class="line">        cv2.rectangle(image, (top, left), (right, bottom), (255, 0, 0), 2)</span><br><span class="line">        cv2.putText(image, &#x27;&#123;0&#125; &#123;1:.2f&#125;&#x27;.format(0, score), (top, left), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2, lineType=cv2.LINE_AA)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@app.route(&#x27;/getdetectresult&#x27;, methods=[&#x27;POST&#x27;])</span><br><span class="line">def getdetectresult():</span><br><span class="line">    notitme=time.time()</span><br><span class="line">    try:</span><br><span class="line">        jsondata = request.json</span><br><span class="line">        print(jsondata)</span><br><span class="line"></span><br><span class="line">        title_img1 = jsondata.get(&quot;title_img1&quot;)</span><br><span class="line">        print(title_img1)</span><br><span class="line">        background_img = jsondata.get(&quot;background_img&quot;)</span><br><span class="line"></span><br><span class="line">        que_img = b64decode(title_img1.split(&#x27;base64,&#x27;)[-1])</span><br><span class="line">        queue = deal_que_new_img(que_img)</span><br><span class="line"></span><br><span class="line">        back_img = b64decode(background_img.split(&#x27;base64,&#x27;)[-1])</span><br><span class="line">        back_img = cv2.imdecode(np.frombuffer(back_img, np.uint8), cv2.IMREAD_COLOR)</span><br><span class="line"></span><br><span class="line">        result = model.infer(back_img).tolist()</span><br><span class="line"></span><br><span class="line">        queid = model.classes.index(queue.split(&quot;个&quot;)[-1])</span><br><span class="line">        # print(result)</span><br><span class="line">        rere = [i for i in result if int(i[5]) == queid]</span><br><span class="line">        rere.sort(key=lambda x: x[2])</span><br><span class="line">        drawdict = rere[-1]</span><br><span class="line"></span><br><span class="line">        result_x = int(drawdict[2] / 640 * back_img.shape[1])</span><br><span class="line">        logger.info(f&quot;&#123;queue&#125;\t&#123;result_x&#125;\t&#123;result&#125;&quot;)</span><br><span class="line">    except Exception as e:</span><br><span class="line">        logger.error(e)</span><br><span class="line">        return &#123;&quot;code&quot;:-1,&quot;msg&quot;:&quot;未识别到&quot;,&quot;data&quot;:[]&#125;</span><br><span class="line">    logger.info(f&quot;耗时：&#123;time.time()-notitme&#125;&quot;)</span><br><span class="line">    return &#123;&quot;code&quot;:0,&quot;msg&quot;:&quot;识别成功&quot;,&quot;data&quot;:&#123;&quot;x&quot;:result_x,&quot;queue&quot;:queue,&quot;result_detect&quot;:result&#125;&#125;</span><br><span class="line"></span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    ocr = DdddOcr(show_ad=False)</span><br><span class="line">    model = YOLOV5_ONNX(onnx_path=&quot;./AliFruit.onnx&quot;)</span><br><span class="line">    app.run(host=&#x27;0.0.0.0&#x27;, port=8848, debug=True)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



</div><div class="article_tags"><span> <i class="yufont icon-tag"></i> 标签: </span><a href="/tags/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/">杂七杂八</a></div></article><div class="article-copyright"><p>标题: <a href="/2024/09/30/Ali_Fruit_Slider_Yolov5/">某里两种版本的水果滑块训练——Yolov5</a></p><p> 链接: <a href="/2024/09/30/Ali_Fruit_Slider_Yolov5/">https://wantoper.github.io/2024/09/30/Ali_Fruit_Slider_Yolov5/</a></p><p> 版权声明: 若无特殊标注皆为 Wantoper 原创版权, 转载请以链接形式注明作者及原始出处</p><p>最后编辑时间: 2024-10-05</p></div><div class="page_turning"><a class="page_up" href="/2024/09/12/label-studio-sam/"
  style="background-image:url(/2024/09/12/label-studio-sam/640.gif)">
  <div class="inner"><span>上一篇:</span>
    <p>部署playground label_studio+sam模型 自动打标注</p>
  </div>
</a>
<a class="page_down" href="/2024/10/03/proxifier-fiddler/"
style="background-image:url(/assets/images/random/16.jpg)">
<div class="inner"><span>下一篇:</span>
  <p>proxifier+fiddler抓包</p>
</div>
</a>
</div></div></div><div class="article_sidebar left"><div class="author_sidebar sidebar_container"><div class="author_info"><div class="avatar"><img class="yu13_img_rotate" src="/assets/imgs/avatar.jpg" alt=""/></div><div class="author_name"><h3><a href="/about">Wantoper</a></h3><p>不断学习，不断提升！</p></div></div><div class="author_data"><ul><li> <div> <span>6</span><p>文章</p></div></li><li> <div> <span>1</span><p>分类</p></div></li><li> <div> <span>1</span><p>标签</p></div></li></ul></div></div><div class="sidebar_tags sidebar_container"><h3 class="sidebar_title"><i class="yufont icon-24gl-folderOpen"></i>分类</h3><div class="archive_lists"><ul class="category-list"><li><a href="/categories/Python/">Python (3)</a></li></ul></div></div><div class="sidebar_tags sidebar_container"><h3 class="sidebar_title"><i class="yufont icon-tag"></i>标签</h3><a href="/tags/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/" style="font-size: 16px;">杂七杂八</a></div><div class="catalogue sidebar_container"><h3 class="sidebar_title"><i class="yufont icon-feiji"></i>目录</h3><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E7%A0%81%E5%AD%97%E4%BD%93%E5%A4%84%E7%9A%84%E8%AF%86%E5%88%AB"><span class="toc-number">2.</span> <span class="toc-text">验证码字体处的识别</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-0%E7%9A%84%E6%96%87%E5%AD%97%E5%A4%84%E7%90%86"><span class="toc-number">2.1.</span> <span class="toc-text">1.0的文字处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-0%E7%9A%84%E6%96%87%E5%AD%97%E5%A4%84%E7%90%86"><span class="toc-number">2.2.</span> <span class="toc-text">2.0的文字处理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%BB%91%E5%9D%97%E5%9B%BE%E7%9A%84%E5%A4%84%E7%90%86%E5%8F%8A%E8%AE%AD%E7%BB%83"><span class="toc-number">3.</span> <span class="toc-text">滑块图的处理及训练</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">4.</span> <span class="toc-text">模型训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%93%E6%A0%87%E6%B3%A8"><span class="toc-number">4.1.</span> <span class="toc-text">打标注</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Yolov5%E7%9A%84%E8%AE%AD%E7%BB%83"><span class="toc-number">4.2.</span> <span class="toc-text">Yolov5的训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%81%E7%A7%BB%E9%83%A8%E7%BD%B2"><span class="toc-number">4.3.</span> <span class="toc-text">迁移部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AC%E4%B8%BAOnnx"><span class="toc-number">4.3.1.</span> <span class="toc-text">转为Onnx</span></a></li></ol></li></ol></li></ol></div></div><script>// 处理滚动事件的函数
function handleScroll() {
    var scrollTop = document.body.scrollTop || document.documentElement.scrollTop;
    var catalogueElement = document.querySelector('.catalogue'); // 使用类名查询元素
    
    if (catalogueElement.dataTop < scrollTop) {
        // 当页面滚动超过目录初始位置时，更新目录位置
        catalogueElement.style.position = 'fixed';
        catalogueElement.style.top = '100px'; // 固定到顶部
        catalogueElement.style.left = catalogueElement.dataLeft; // 保持原来的水平位置
    } else {
        // 恢复目录到原始位置
        catalogueElement.style.position = 'static';
    }
}

// 计算元素的初始位置并存储
function setInitialPosition(element) {
    var offsetTop = element.offsetTop;
    var offsetLeft = element.offsetLeft;

    // 计算元素相对于文档的位置
    var currentElement = element; // 保存原始元素引用
    while ((currentElement = currentElement.offsetParent)) {
        offsetTop += currentElement.offsetTop;
        offsetLeft += currentElement.offsetLeft;
    }
    // 将计算好的位置存储在元素的自定义属性中
    element.dataTop = offsetTop;
    element.dataLeft = offsetLeft;
}

// 页面加载完成后执行
window.onload = function() {
    var catalogueElement = document.querySelector('.catalogue'); // 使用类名查询元素
    setInitialPosition(catalogueElement); // 设置目录元素的初始位置
    window.onscroll = handleScroll; // 绑定滚动事件
};</script></div><footer class="footer"> <div class="footer_bottom"><div class="footer_box"><div class="copy"><span>主题</span><a target="_blank" rel="noopener" href="https://hexo.io/zh-cn">Hexo</a><span>模板</span><a href="/">Wantoper</a></div><div class="copy"><span>© 2022<a href="/">Wantoper</a>暂无备案号</span></div></div></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.1/jquery.min.js"> </script><script src="/assets/lib/prettify/prettify.js"> </script><script src="/assets/lib/fancybox/fancybox.js"> </script><script src="/assets/js/app.js"> </script></div></body></html>